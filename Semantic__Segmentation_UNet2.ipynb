{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic _Segmentation_UNet2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB8S1zN1Rrdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount ('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRw60CqpL8zY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_PATH= \"/content/drive/My Drive//PascalVOC2012_Dataset/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGtym3_XL9Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Xm4NRSL-0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data as data\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "class VOCSegmentation(data.Dataset):\n",
        "    def __init__(self, root, image_set, transform=None, target_transform=None):\n",
        "        self.root = root\n",
        "        self.image_set = image_set\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self._annopath = os.path.join(self.root, 'SegmentationClass', '%s.png')\n",
        "        self._imgpath = os.path.join(self.root, 'JPEGImages', '%s.jpg')\n",
        "        self._imgsetpath = os.path.join(self.root, 'ImageSets', 'Segmentation', '%s.txt')\n",
        " \n",
        "        with open(self._imgsetpath % self.image_set) as f:\n",
        "            self.ids = f.readlines()\n",
        "        self.ids = [x.strip('\\n') for x in self.ids]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "\n",
        "        target = Image.open(self._annopath % img_id)#.convert('RGB')\n",
        "\n",
        "        img = Image.open(self._imgpath % img_id).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkQXuyYrMEWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([transforms.Resize((128, 128)),transforms.ToTensor()]) \n",
        "\n",
        "test_transform = transforms.Compose ([transforms.Resize((128,128)), transforms.ToTensor()])  \n",
        "\n",
        "training_set = VOCSegmentation(root= PROJECT_PATH, image_set = 'train', transform= test_transform, target_transform= test_transform)\n",
        "testing_set = VOCSegmentation(root= PROJECT_PATH, image_set = 'val', transform= test_transform, target_transform= test_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd7FjNa-MG-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"trainset length: {len(training_set)}\")\n",
        "print(f\"valset length: {len(testing_set)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT8n17f_MJH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set.ids = training_set.ids\n",
        "#training_set.ids = training_set.ids[:16]\n",
        "len(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dpoxHvRMOFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(training_set, batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(testing_set, batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "#get first batch of the dataset\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "\n",
        "batch_index = 0\n",
        "\n",
        "print(images[batch_index].shape)\n",
        "print(labels[batch_index].shape)\n",
        "print(images.shape)\n",
        "#torch.set_printoptions(profile=\"full\")\n",
        "#torch.set_printoptions(edgeitems=3)\n",
        "#print(labels[0]*255)  \n",
        "\n",
        "def show(img):\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)), interpolation='nearest')\n",
        "\n",
        "def show_gray(img):\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)), interpolation='nearest', cmap='gray')\n",
        "\n",
        "label_rgb =  labels[batch_index]\n",
        "label_rgb = torch.cat([label_rgb,label_rgb,label_rgb], dim=0)\n",
        "\n",
        "\n",
        "print(label_rgb.shape)\n",
        "\n",
        "\n",
        "show(images[0])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRRDRROmMPVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cmap(n):\n",
        "    if (n is None) or (n == 0):\n",
        "        n = 255\n",
        "\n",
        "    cmap_array = np.zeros((n, 3))\n",
        "    for i in range(n):\n",
        "        id = i\n",
        "        r = 0\n",
        "        g = 0\n",
        "        b = 0\n",
        "        for j in range(7):\n",
        "            bgr_string = np.binary_repr(id)\n",
        "            bgr = int(bgr_string[-1])\n",
        "            bsr = np.left_shift(bgr, 7 - j)\n",
        "            r = np.bitwise_or(r, bsr)\n",
        "\n",
        "            bgg_string = np.binary_repr(id)\n",
        "            if len(bgg_string) > 1:\n",
        "                bgg = int(bgg_string[-2])\n",
        "            else:\n",
        "                bgg = 0\n",
        "            bsg = np.left_shift(bgg, 7 - j)\n",
        "            g = np.bitwise_or(g, bsg)\n",
        "\n",
        "            bgb_string = np.binary_repr(id)\n",
        "            if len(bgb_string) > 2:\n",
        "                bgb = int(bgb_string[-3])\n",
        "            else:\n",
        "                bgb = 0\n",
        "            bsb = np.left_shift(bgb, 7 - j)\n",
        "            b = np.bitwise_or(b, bsb)\n",
        "\n",
        "            id = np.right_shift(id, 3)\n",
        "        cmap_array[i, 0] = r\n",
        "        cmap_array[i, 1] = g\n",
        "        cmap_array[i, 2] = b\n",
        "\n",
        "    return cmap_array\n",
        "\n",
        "cmap_array_gen = cmap(21)\n",
        "print(\"1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFw74c8HMTAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rgb_img_from_raw_tensor(input_tensor, batch_index):\n",
        "  \"\"\"\n",
        "  logic to convert to rgb image:\n",
        "  Args:\n",
        "\n",
        "    input_tensor:  Tensor [B,H,W] with values between [0-19,255] PASCALVOC2007 format\n",
        "    batch_index: convert index to rgb from batch (B available images)\n",
        "  \"\"\"\n",
        "\n",
        "  rgb_tensor = np.zeros((128, 128, 3))\n",
        "  #print(rgb_tensor.shape)\n",
        "  for i in range(128):\n",
        "      for j in range(128):\n",
        "          rgb = cmap_array_gen[input_tensor[batch_index, i, j]]\n",
        "          rgb_tensor[i, j, :] = np.flipud(rgb)\n",
        "\n",
        "  #print(rgb_tensor.shape)\n",
        "\n",
        "  pil_image = Image.fromarray(np.uint8(rgb_tensor))\n",
        "  #print(pil_image.size)\n",
        "  return pil_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jteca-yTMYYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_labels(labels):\n",
        "  \"\"\"\n",
        "  recover [0-20 and 255] initial gt format from [0-1] by multiplying with 255\n",
        "  remove additional dimension at position 1 (channel) using squeeze  : B,1,H,W (dataloader) ->B,H,W (format expected by crossentropy clsindex)\n",
        "  Args:\n",
        "\n",
        "    input_tensor:  Tensor [B,H,W] with values between [0-19,255] PASCALVOC2007 format\n",
        "    batch_index: convert index to rgb from batch (B available images)\n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "  #print(f\"process_labels before {labels.shape}\")\n",
        "  labels=labels.squeeze(1)*255\n",
        "  #print(f\"process_labels after squeeze {labels.shape}\")\n",
        "  labels = labels.type(torch.LongTensor)\n",
        "  #void pixels (boundary region) is considered background\n",
        "  undefined_indices = labels == 255 \n",
        "  labels[undefined_indices] = 0\n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9s1cbtRMbIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = process_labels(labels)\n",
        "pil_target_image = get_rgb_img_from_raw_tensor(labels,batch_index)\n",
        "pil_target_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TttebCKBMdzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show(images[batch_index]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4lmdlVMeWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_gray(label_rgb) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDt6YCK_MlZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution Block \n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(conv_block, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up_conv(nn.Module):\n",
        "    \"\"\"\n",
        "    Up Convolution Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(up_conv, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class U_Net(nn.Module):\n",
        "    \"\"\"\n",
        "    UNet - Basic Implementation\n",
        "    Paper : https://arxiv.org/abs/1505.04597\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=24):\n",
        "        super(U_Net, self).__init__()\n",
        "\n",
        "        n1 = 32\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8]\n",
        "        \n",
        "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(in_ch, filters[0])\n",
        "        self.Conv2 = conv_block(filters[0], filters[1])\n",
        "        self.Conv3 = conv_block(filters[1], filters[2])\n",
        "        self.Conv4 = conv_block(filters[2], filters[3])\n",
        "\n",
        "\n",
        "\n",
        "        self.Up4 = up_conv(filters[3], filters[2])\n",
        "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
        "\n",
        "        self.Up3 = up_conv(filters[2], filters[1])\n",
        "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
        "\n",
        "        self.Up2 = up_conv(filters[1], filters[0])\n",
        "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
        "\n",
        "        self.Conv = nn.Conv2d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "       # self.active = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        e1 = self.Conv1(x)\n",
        "\n",
        "        e2 = self.Maxpool1(e1)\n",
        "        e2 = self.Conv2(e2)\n",
        "\n",
        "        e3 = self.Maxpool2(e2)\n",
        "        e3 = self.Conv3(e3)\n",
        "\n",
        "        e4 = self.Maxpool3(e3)\n",
        "        e4 = self.Conv4(e4)\n",
        "\n",
        "\n",
        "\n",
        "        d4 = self.Up4(e4)\n",
        "        d4 = torch.cat((e3, d4), dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((e2, d3), dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((e1, d2), dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        out = self.Conv(d2)\n",
        "\n",
        "        #d1 = self.active(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "image = torch.rand((1, 3, 128, 128))\n",
        "model = U_Net(3,21)\n",
        "out = model(image)\n",
        "print(out.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIGR4eTLMoV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 128, 128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVnPU6-6MqNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "import torch.nn \n",
        "import multiprocessing\n",
        "\n",
        "\n",
        "num_worker=int(multiprocessing.cpu_count()/2)\n",
        "if torch.cuda.is_available():\n",
        "   device = torch.device('cuda')\n",
        "   torch.cuda.empty_cache()\n",
        "   print(\"Cuda\")\n",
        "\n",
        "else:\n",
        "   device = torch.device('cpu')\n",
        "\n",
        "# model = Unet(nclasses)\n",
        "# if os.path.exists(Model_Path):\n",
        "#    model.load_state_dict(torch.load(Model_Path))\n",
        "\n",
        "# model=model.to(device)\n",
        "# optimizer = torch.optim.SGD(model.parameters(),lr = Lr, momdentum=momentum)\n",
        "# criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or3DG4ZAMs_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAVEPATH = '/content/drive/My Drive//ModelSaved/model_4.pth'\n",
        "\n",
        "def train_model(model, dataloaders, optimizer, scheduler, device, num_epochs=5):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    avg_best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "      \n",
        "        # Each epoch has a training and validation phase\n",
        "        phases=['train']\n",
        "        if 'val' in dataloaders.keys():\n",
        "          phases.append('val')\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for iteration,(inputs, labels) in enumerate(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = process_labels(labels)\n",
        "                # #recover [0-20 and 255] initial gt format from [0-1] by multiplying with 255\n",
        "                # #remove additional dimension at position 1 (channel) using squeeze  : B,1,H,W (dataloader) ->B,H,W (crossentropy loss format clsindex)\n",
        "                # labels = labels.squeeze(1)*255\n",
        "                # labels = labels.type(torch.LongTensor)\n",
        "                # #void pixels (boundary region) is considered background\n",
        "                # labels[labels == 255] = 0\n",
        "                \n",
        "                labels = labels.to(device)\n",
        "\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    print(outputs.shape)\n",
        "                    print(labels.shape)\n",
        "                    loss = calc_loss(outputs, labels, metrics)  #labels is clsindex for crossentropy loss [0-19]\n",
        "                    #if iteration%10==0:\n",
        "                    print(f\"{phase}_loss at epoch :{epoch}/ iteration: {iteration}/{len(dataloaders[phase])} loss: {loss}\")\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        print(\"LR\", param_group['lr'])\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0)\n",
        "\n",
        "            print(f\"{phase}_loss at end of epoch :{epoch}  loss: {loss}\")\n",
        "            #print_metrics(metrics, epoch_samples, phase)\n",
        "            #AVERAGE LOSS Metric (for validation) PER EPOCH\n",
        "            avg_epoch_loss = metrics['loss'] / epoch_samples\n",
        "\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and avg_epoch_loss < avg_best_loss:\n",
        "                print(\"saving best model\")\n",
        "                avg_best_loss = avg_epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                #torch.save(model.state_dict(), SAVEPATH)\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "        if epoch % 5==0:\n",
        "          torch.save(model.state_dict(), SAVEPATH)\n",
        "          print(\"saving model pth\")\n",
        "    print('Best val loss: {:4f}'.format(avg_best_loss))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9HvmUhfMwvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import collections\n",
        "d = collections.defaultdict(list)\n",
        "\n",
        "# def dice_loss(pred, target, smooth = 1.):\n",
        "#     pred = pred.contiguous()\n",
        "#     target = target.contiguous()    \n",
        "\n",
        "#     intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    \n",
        "#     loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
        "    \n",
        "#     return loss.mean()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "\n",
        "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
        "    \n",
        "    CCE_loss = criterion(pred, target)\n",
        "    metrics['CCE'] += CCE_loss.data.cpu().numpy() * target.size(0)\n",
        "\n",
        "    return CCE_loss\n",
        "\n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "        \n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoqCotm5M1ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "num_classes = 21\n",
        "#model = Unet1(num_classes).to(device)\n",
        "#model = UNet2(3,num_classes).to(device)\n",
        "model = U_Net(3,num_classes).to(device)\n",
        "\n",
        "optimizer_ft = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-2)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.1)\n",
        "\n",
        "#dataloaders={'train':train_loader, 'val':val_loader}\n",
        "\n",
        "#for overfitting on truncated dataset do not validate after each epoch\n",
        "dataloaders={'train':train_loader}\n",
        "print(dataloaders)\n",
        "model = train_model(model, dataloaders, optimizer_ft, exp_lr_scheduler, device=device, num_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lq6Csn2M2Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloaders={'train':train_loader}\n",
        "'val' in dataloaders.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YVZjgrcM5mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testiter = iter(val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D69MFDVUM7CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainiter = iter(train_loader)\n",
        "print(len(train_loader.dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfK2iD9nM8ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get first batch of the dataset\n",
        "#images, labels = testiter.next()\n",
        "images, labels = trainiter.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OYzrDvQM_2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print input\n",
        "val_images=images\n",
        "print(val_images.shape)\n",
        "show(val_images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tHva3wQNCpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print target \n",
        "print(labels.shape)\n",
        "labels = process_labels(labels)\n",
        "print(labels.shape)\n",
        "rgb_image=get_rgb_img_from_raw_tensor(labels,0)\n",
        "rgb_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EgJnK5eNDLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labels[0,100,100])\n",
        "print(labels[0,120,35])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m8mzvI6NF-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading model from Drive\n",
        "SAVEPATH = '/content/drive/My Drive//ModelSaved/model_2.pth'\n",
        "import os.path\n",
        "from os import path\n",
        "print(path.exists(SAVEPATH))\n",
        "model = U_Net(3,21)\n",
        "\n",
        "model.load_state_dict(torch.load(SAVEPATH))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxrBNysrNL8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add batch dimension\n",
        "#val_image=val_image.unsqueeze(0)\n",
        "\n",
        "#torch load(mo...dsdsta path)\n",
        "print(val_images.shape)\n",
        "out=model(val_images) #B,21,H,W ->B,1,H,W->B,H,W   [0,0,0,0,1,00,0,...0] -> 5\n",
        "#softmaxlayer = nn.Softmax(dim=1)\n",
        "#out=softmaxlayer(out) #maybe not necessary- because does not change argmax result \n",
        "print(out.shape)\n",
        "out_argmax= torch.argmax(out,1)\n",
        "print(out_argmax.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5vR8JHjNOXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print output\n",
        "rgb_image=get_rgb_img_from_raw_tensor(out_argmax,0)\n",
        "rgb_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMqgC5rZNQUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.max(out)\n",
        "print(out[0,:,100,100])\n",
        "print(out_argmax[0,100,100])\n",
        "\n",
        "print(out[0,:,20,60])\n",
        "print(out_argmax[0,20,60])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}