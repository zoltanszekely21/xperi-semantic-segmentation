{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic_Segmentation_UNet1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHSstcSf01mJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount ('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlhpmFIi1BMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_PATH= \"/content/drive/My Drive//PascalVOC2012_Dataset/\"  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnEXEs7HxGmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjdpoG8KxSIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_transform = transforms.Compose([transforms.Resize((128, 128),transforms.ToTensor())]) \n",
        "\n",
        "# test_transform = transforms.Compose ([transforms.Resize((128,128), transforms.ToTensor())])  \n",
        "\n",
        "# train = datasets.VOCSegmentation(root= 'data', image_set='train', download=True, transform=train_transform, target_transform=train_transform, transforms=None)\n",
        "# test= datasets.VOCSegmentation(root= 'data', image_set='val', download=True, transform=test_transform, target_transform=test_transform, transforms=None)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwXuLIoYwhba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data as data\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "class VOCSegmentation(data.Dataset):\n",
        "    def __init__(self, root, image_set, transform=None, target_transform=None):\n",
        "        self.root = root\n",
        "        self.image_set = image_set\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self._annopath = os.path.join(self.root, 'SegmentationClass', '%s.png')\n",
        "        self._imgpath = os.path.join(self.root, 'JPEGImages', '%s.jpg')\n",
        "        self._imgsetpath = os.path.join(self.root, 'ImageSets', 'Segmentation', '%s.txt')\n",
        " \n",
        "        with open(self._imgsetpath % self.image_set) as f:\n",
        "            self.ids = f.readlines()\n",
        "        self.ids = [x.strip('\\n') for x in self.ids]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "\n",
        "        target = Image.open(self._annopath % img_id)#.convert('RGB')\n",
        "\n",
        "        img = Image.open(self._imgpath % img_id).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1XaqPhju-LU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([transforms.Resize((128, 128)),transforms.ToTensor()]) \n",
        "\n",
        "test_transform = transforms.Compose ([transforms.Resize((128,128)), transforms.ToTensor()])  \n",
        "\n",
        "training_set = VOCSegmentation(root= PROJECT_PATH, image_set = 'train', transform= test_transform, target_transform= test_transform)\n",
        "testing_set = VOCSegmentation(root= PROJECT_PATH, image_set = 'val', transform= test_transform, target_transform= test_transform)\n",
        "\n",
        "#training_set = datasets.VOCSegmentation(root= 'data', image_set = 'train', download=True, transform= train_transform, target_transform= train_transform)\n",
        "#testing_set = datasets.VOCSegmentation(root= 'data', image_set = 'val', download=True, transform= test_transform, target_transform= test_transform)\n",
        "\n",
        "# x,y=training_set[0]\n",
        "# print(x[0,0:4,0:4])\n",
        "# print(y[0,0:4,0:4])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5HLpWlSvhWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(f\"trainset length: {len(training_set)}\")\n",
        "print(f\"valset length: {len(testing_set)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEJdMdYexkNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training_set.ids = training_set.ids\n",
        "training_set.ids = training_set.ids[:16]\n",
        "len(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSmf_kjoUHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(training_set, batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(testing_set, batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "#get first batch of the dataset\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "\n",
        "batch_index = 0\n",
        "\n",
        "print(images[batch_index].shape)\n",
        "print(labels[batch_index].shape)\n",
        "print(images.shape)\n",
        "#torch.set_printoptions(profile=\"full\")\n",
        "#torch.set_printoptions(edgeitems=3)\n",
        "#print(labels[0]*255)  \n",
        "\n",
        "def show(img):\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)), interpolation='nearest')\n",
        "\n",
        "def show_gray(img):\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)), interpolation='nearest', cmap='gray')\n",
        "\n",
        "label_rgb =  labels[batch_index]\n",
        "label_rgb = torch.cat([label_rgb,label_rgb,label_rgb], dim=0)\n",
        "\n",
        "\n",
        "print(label_rgb.shape)\n",
        "\n",
        "\n",
        "show(images[0])  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLyYRTwnguJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def cmap(n):\n",
        "    if (n is None) or (n == 0):\n",
        "        n = 255\n",
        "\n",
        "    cmap_array = np.zeros((n, 3))\n",
        "    for i in range(n):\n",
        "        id = i\n",
        "        r = 0\n",
        "        g = 0\n",
        "        b = 0\n",
        "        for j in range(7):\n",
        "            bgr_string = np.binary_repr(id)\n",
        "            bgr = int(bgr_string[-1])\n",
        "            bsr = np.left_shift(bgr, 7 - j)\n",
        "            r = np.bitwise_or(r, bsr)\n",
        "\n",
        "            bgg_string = np.binary_repr(id)\n",
        "            if len(bgg_string) > 1:\n",
        "                bgg = int(bgg_string[-2])\n",
        "            else:\n",
        "                bgg = 0\n",
        "            bsg = np.left_shift(bgg, 7 - j)\n",
        "            g = np.bitwise_or(g, bsg)\n",
        "\n",
        "            bgb_string = np.binary_repr(id)\n",
        "            if len(bgb_string) > 2:\n",
        "                bgb = int(bgb_string[-3])\n",
        "            else:\n",
        "                bgb = 0\n",
        "            bsb = np.left_shift(bgb, 7 - j)\n",
        "            b = np.bitwise_or(b, bsb)\n",
        "\n",
        "            id = np.right_shift(id, 3)\n",
        "        cmap_array[i, 0] = r\n",
        "        cmap_array[i, 1] = g\n",
        "        cmap_array[i, 2] = b\n",
        "\n",
        "    return cmap_array\n",
        "\n",
        "cmap_array_gen = cmap(21)\n",
        "print(\"1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewj30dvZgbcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def get_rgb_img_from_raw_tensor(input_tensor, batch_index):\n",
        "  \"\"\"\n",
        "  logic to convert to rgb image:\n",
        "  Args:\n",
        "\n",
        "    input_tensor:  Tensor [B,H,W] with values between [0-19,255] PASCALVOC2007 format\n",
        "    batch_index: convert index to rgb from batch (B available images)\n",
        "  \"\"\"\n",
        "\n",
        "  rgb_tensor = np.zeros((128, 128, 3))\n",
        "  #print(rgb_tensor.shape)\n",
        "  for i in range(128):\n",
        "      for j in range(128):\n",
        "          rgb = cmap_array_gen[input_tensor[batch_index, i, j]]\n",
        "          rgb_tensor[i, j, :] = np.flipud(rgb)\n",
        "\n",
        "  #print(rgb_tensor.shape)\n",
        "\n",
        "  pil_image = Image.fromarray(np.uint8(rgb_tensor))\n",
        "  #print(pil_image.size)\n",
        "  return pil_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HddIVE0q849L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_labels(labels):\n",
        "  \"\"\"\n",
        "  recover [0-20 and 255] initial gt format from [0-1] by multiplying with 255\n",
        "  remove additional dimension at position 1 (channel) using squeeze  : B,1,H,W (dataloader) ->B,H,W (format expected by crossentropy clsindex)\n",
        "  Args:\n",
        "\n",
        "    input_tensor:  Tensor [B,H,W] with values between [0-19,255] PASCALVOC2007 format\n",
        "    batch_index: convert index to rgb from batch (B available images)\n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "  #print(f\"process_labels before {labels.shape}\")\n",
        "  labels=labels.squeeze(1)*255\n",
        "  #print(f\"process_labels after squeeze {labels.shape}\")\n",
        "  labels = labels.type(torch.LongTensor)\n",
        "  #void pixels (boundary region) is considered background\n",
        "  undefined_indices = labels == 255 \n",
        "  labels[undefined_indices] = 0\n",
        "  return labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7-tGWImi8t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "labels = process_labels(labels)\n",
        "pil_target_image = get_rgb_img_from_raw_tensor(labels,batch_index)\n",
        "pil_target_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoiX8ByigEfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show(images[batch_index]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsC1zbrDgCOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_gray(label_rgb) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt4Hjjwr8OSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 24)\n",
        "        self.down1 = Down(24, 48)\n",
        "        self.down2 = Down(48, 96)\n",
        "        self.down3 = Down(96, 192)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(192, 384 // factor)\n",
        "        self.up1 = Up(384, 192 // factor, bilinear)\n",
        "        self.up2 = Up(192, 96 // factor, bilinear)\n",
        "        self.up3 = Up(96, 48 // factor, bilinear)\n",
        "        self.up4 = Up(48, 24, bilinear)\n",
        "        self.outc = OutConv(24, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "    \n",
        "image = torch.rand((1, 3, 128, 128))\n",
        "model = UNet(3,21)\n",
        "out = model(image)\n",
        "print(out.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7hFjlja_bzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 128, 128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1AO5dswFGmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "import torch.nn \n",
        "import multiprocessing\n",
        "\n",
        "\n",
        "num_worker=int(multiprocessing.cpu_count()/2)\n",
        "if torch.cuda.is_available():\n",
        "   device = torch.device('cuda')\n",
        "   torch.cuda.empty_cache()\n",
        "   print(\"Cuda\")\n",
        "\n",
        "else:\n",
        "   device = torch.device('cpu')\n",
        "\n",
        "# model = Unet(nclasses)\n",
        "# if os.path.exists(Model_Path):\n",
        "#    model.load_state_dict(torch.load(Model_Path))\n",
        "\n",
        "# model=model.to(device)\n",
        "# optimizer = torch.optim.SGD(model.parameters(),lr = Lr, momdentum=momentum)\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynqIJlx1369o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAVEPATH = '/content/drive/My Drive//ModelSaved/model_3.pth'\n",
        "\n",
        "def train_model(model, dataloaders, optimizer, scheduler, device, num_epochs=200):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    avg_best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "      \n",
        "        # Each epoch has a training and validation phase\n",
        "        phases=['train']\n",
        "        if 'val' in dataloaders.keys():\n",
        "          phases.append('val')\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for iteration,(inputs, labels) in enumerate(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = process_labels(labels)\n",
        "                # #recover [0-20 and 255] initial gt format from [0-1] by multiplying with 255\n",
        "                # #remove additional dimension at position 1 (channel) using squeeze  : B,1,H,W (dataloader) ->B,H,W (crossentropy loss format clsindex)\n",
        "                # labels = labels.squeeze(1)*255\n",
        "                # labels = labels.type(torch.LongTensor)\n",
        "                # #void pixels (boundary region) is considered background\n",
        "                # labels[labels == 255] = 0\n",
        "                \n",
        "                labels = labels.to(device)\n",
        "\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    print(outputs.shape)\n",
        "                    print(labels.shape)\n",
        "                    loss = calc_loss(outputs, labels, metrics)  #labels is clsindex for crossentropy loss [0-19]\n",
        "                    #if iteration%10==0:\n",
        "                    print(f\"{phase}_loss at epoch :{epoch}/ iteration: {iteration}/{len(dataloaders[phase])} loss: {loss}\")\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        print(\"LR\", param_group['lr'])\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0)\n",
        "\n",
        "            print(f\"{phase}_loss at end of epoch :{epoch}  loss: {loss}\")\n",
        "            #print_metrics(metrics, epoch_samples, phase)\n",
        "            #AVERAGE LOSS Metric (for validation) PER EPOCH\n",
        "            avg_epoch_loss = metrics['loss'] / epoch_samples\n",
        "\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and avg_epoch_loss < avg_best_loss:\n",
        "                print(\"saving best model\")\n",
        "                avg_best_loss = avg_epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                #torch.save(model.state_dict(), SAVEPATH)\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "        if epoch % 5==0:\n",
        "          torch.save(model.state_dict(), SAVEPATH)\n",
        "          print(\"saving model pth\")\n",
        "    print('Best val loss: {:4f}'.format(avg_best_loss))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Gdhgr-Mmt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import collections\n",
        "d = collections.defaultdict(list)\n",
        "\n",
        "# def dice_loss(pred, target, smooth = 1.):\n",
        "#     pred = pred.contiguous()\n",
        "#     target = target.contiguous()    \n",
        "\n",
        "#     intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    \n",
        "#     loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
        "    \n",
        "#     return loss.mean()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "\n",
        "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
        "    \n",
        "    CCE_loss = criterion(pred, target)\n",
        "    metrics['CCE'] += CCE_loss.data.cpu().numpy() * target.size(0)\n",
        "\n",
        "    return CCE_loss\n",
        "\n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "        \n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgSULZ0N4KIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "num_classes = 21\n",
        "#model = Unet1(num_classes).to(device)\n",
        "model = UNet(3,num_classes).to(device)\n",
        "#model = U_Net3(3,num_classes).to(device)\n",
        "\n",
        "optimizer_ft = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-2)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=50, gamma=0.1)\n",
        "\n",
        "#dataloaders={'train':train_loader, 'val':val_loader}\n",
        "\n",
        "#for overfitting on truncated dataset do not validate after each epoch\n",
        "dataloaders={'train':train_loader}\n",
        "print(dataloaders)\n",
        "model = train_model(model, dataloaders, optimizer_ft, exp_lr_scheduler, device=device, num_epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pG92cdz0liU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloaders={'train':train_loader}\n",
        "'val' in dataloaders.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YWBmV_O10mP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testiter = iter(val_loader)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7BMaJzI6aXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainiter = iter(train_loader)\n",
        "print(len(train_loader.dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ecnmuxAi8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get first batch of the dataset\n",
        "#images, labels = testiter.next()\n",
        "images, labels = trainiter.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Gw_7vp4FdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print input\n",
        "val_images=images\n",
        "print(val_images.shape)\n",
        "show(val_images[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxL7FJRs_Jj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print target \n",
        "print(labels.shape)\n",
        "labels = process_labels(labels)\n",
        "print(labels.shape)\n",
        "rgb_image=get_rgb_img_from_raw_tensor(labels,0)\n",
        "rgb_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z28B8vh8oRKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labels[0,100,100])\n",
        "print(labels[0,120,35])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qsLlhbGUYF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#laoding model from Drive\n",
        "SAVEPATH = '/content/drive/My Drive//ModelSaved/model_3.pth'\n",
        "import os.path\n",
        "from os import path\n",
        "print(path.exists(SAVEPATH))\n",
        "model = UNet(3,21)\n",
        "\n",
        "model.load_state_dict(torch.load(SAVEPATH))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUgXYShY4Hq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add batch dimension\n",
        "#val_image=val_image.unsqueeze(0)\n",
        "\n",
        "#torch load(mo...dsdsta path)\n",
        "print(val_images.shape)\n",
        "out=model(val_images) #B,21,H,W ->B,1,H,W->B,H,W   [0,0,0,0,1,00,0,...0] -> 5\n",
        "#softmaxlayer = nn.Softmax(dim=1)\n",
        "#out=softmaxlayer(out) #maybe not necessary- because does not change argmax result \n",
        "print(out.shape)\n",
        "out_argmax= torch.argmax(out,1)\n",
        "print(out_argmax.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJw2WLhX4I8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print output\n",
        "rgb_image=get_rgb_img_from_raw_tensor(out_argmax,0)\n",
        "rgb_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKxP2tnu6iB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.max(out)\n",
        "print(out[0,:,100,100])\n",
        "print(out_argmax[0,100,100])\n",
        "\n",
        "print(out[0,:,20,60])\n",
        "print(out_argmax[0,20,60])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}